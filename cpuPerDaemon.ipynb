{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download pyhive\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} pyhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import re\n",
    "import datetime as dt\n",
    "from pyhive import hive\n",
    "from operator import itemgetter\n",
    "\n",
    "hiveTraceTableName = 'trace_avro_v3'\n",
    "\n",
    "# HiveServer2 Status http://sayonara1a-mnds1-2-prd.eng.sfdc.net:10002/\n",
    "# Haddop All Apps Status http://sayonara1a-mnds1-2-prd.eng.sfdc.net:8088/cluster/set mapred.job.name = Job: condSql.dailyFieldsNeeded;\n",
    "def getHiveConnection():\n",
    "    sdbKP = 'sayonara1a-mnds1-2-prd.eng.sfdc.net'\n",
    "    conn = hive.connect(sdbKP, \n",
    "                        configuration={'hive.fetch.task.conversion': 'none'})\n",
    "    return(conn)\n",
    "def createHql(minDate=None,\n",
    "              maxDate=None,\n",
    "              hostnames=[]):\n",
    "    hql = '''\n",
    "set hive.fetch.task.conversion=none;\n",
    "\n",
    "DROP TABLE IF EXISTS et_timerMap;\n",
    "DROP TABLE IF EXISTS et_tmp_bes_17;\n",
    "DROP TABLE IF EXISTS et_tmp_e2e_time;\n",
    "DROP TABLE IF EXISTS et_tmp_e2e_catMap;\n",
    "DROP TABLE IF EXISTS et_tmp_e2e_catMapSum;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS et_timerMap (categoryName STRING, metricName STRING);\n",
    "INSERT INTO et_timerMap VALUES \n",
    "('cpu_time', 'main'),\n",
    "('wait_app_time', 'recv'),\n",
    "('wait_app_time', 'recv_command_wait'),\n",
    "('wait_app_time', 'send'),\n",
    "('cpu_time', 'process_msg'),\n",
    "('cpu_time', 'simple_query'),\n",
    "('cpu_time', 'parse_message'),\n",
    "('cpu_time', 'describe_message'),\n",
    "('cpu_time', 'bind_message'),\n",
    "('cpu_time', 'execute_message'),\n",
    "('cpu_time', 'close_message'),\n",
    "('cpu_time', 'parse'),\n",
    "('cpu_time', 'qAnalyze'),\n",
    "('cpu_time', 'rewrite'),\n",
    "('cpu_time', 'plan'),\n",
    "('cpu_time', 'spl_overhead'),\n",
    "('cpu_time', 'spl_body'),\n",
    "('cpu_time', 'spl_eval_expr'),\n",
    "('cpu_time', 'spl_eval_expr_trivial'),\n",
    "('cpu_time', 'spl_eval_expr_simple'),\n",
    "('cpu_time', 'spl_eval_expr_simple_init'),\n",
    "('cpu_time', 'spl_eval_expr_simple_run'),\n",
    "('cpu_time', 'spl_eval_expr_complex'),\n",
    "('cpu_time', 'spl_compile'),\n",
    "('cpu_time', 'executor_start'),\n",
    "('cpu_time', 'executor_start_init_plan'),\n",
    "('cpu_time', 'executor_start_serviceability'),\n",
    "('cpu_time', 'executor_run'),\n",
    "('cpu_time', 'executor_finish'),\n",
    "('cpu_time', 'executor_end'),\n",
    "('cpu_time', 'executor_end_serviceability'),\n",
    "('cpu_time', 'executor_rewind'),\n",
    "('cpu_time', 'SPI_execute'),\n",
    "('cpu_time', 'SPI_prepare'),\n",
    "('cpu_time', 'SPI_open'),\n",
    "('cpu_time', 'SPI_fetch'),\n",
    "('cpu_time', 'SPI_close'),\n",
    "('cpu_time', 'SPI_misc'),\n",
    "('cpu_time', 'SPI_free'),\n",
    "('cpu_time', 'SPI_save'),\n",
    "('cpu_time', 'plancache_getcached'),\n",
    "('cpu_time', 'plancache_build'),\n",
    "('cpu_time', 'plancache_revalidate'),\n",
    "('cpu_time', 'plancache_planmove'),\n",
    "('cpu_time', 'plancache_psrcmove'),\n",
    "('cpu_time', 'plancache_publish'),\n",
    "('cpu_time', 'lsm'),\n",
    "('cpu_time', 'xlog_cpu_time'),\n",
    "('wait_time', 'xlog_group_commit_wait'),\n",
    "('wait_time', 'wait_misc'),\n",
    "('wait_time', 'wait_sql_lock'),\n",
    "('wait_time', 'wait_AEAM_lock'),\n",
    "('wait_time', 'wait_EIRQ_lock'),\n",
    "('wait_time', 'wait_MXP_lock'),\n",
    "('io_time', 'store_ext_create'),\n",
    "('io_time', 'store_ext_delete'),\n",
    "('io_time', 'store_ext_list'),\n",
    "('io_time', 'store_ext_stat'),\n",
    "('io_time', 'store_ext_open'),\n",
    "('io_time', 'store_ext_seek'),\n",
    "('io_time', 'store_log_sync_write'),\n",
    "('io_time', 'store_data_sync_write'),\n",
    "('io_time', 'store_cat_sync_write'),\n",
    "('io_time', 'store_seq_read'),\n",
    "('io_time', 'store_rand_read'),\n",
    "('io_time', 'store_ext_close'),\n",
    "('cpu_time', 'tuple_sort'),\n",
    "('wait_time', 'wait_memstore_throttle'),\n",
    "('wait_time', 'temp_file_read'),\n",
    "('wait_time', 'temp_file_write'),\n",
    "('io_time', 'store_log_async_submit'),\n",
    "('io_time', 'store_log_async_status'),\n",
    "('io_time', 'store_data_async_submit'),\n",
    "('io_time', 'store_data_async_status'),\n",
    "('io_time', 'store_cat_async_submit'),\n",
    "('io_time', 'store_cat_async_status'),\n",
    "('cpu_time', 'lsm_insert'),\n",
    "('cpu_time', 'lsm_insert_snapshot_check'),\n",
    "('cpu_time', 'lsm_insert_dup_check_primary'),\n",
    "('cpu_time', 'lsm_insert_dup_check_secondary'),\n",
    "('cpu_time', 'lsm_scan'),\n",
    "('cpu_time', 'lsm_fetch'),\n",
    "('cpu_time', 'memstore_lock'),\n",
    "('cpu_time', 'memstore_insert'),\n",
    "('cpu_time', 'memstore_fetch'),\n",
    "('cpu_time', 'memstore_scan'),\n",
    "('cpu_time', 'seq_nextval'),\n",
    "('wait_time', 'wait_BufReadIO_lock'),\n",
    "('ext_time', 'cluuid_read'),\n",
    "('ext_time', 'cluuid_write'),\n",
    "('ext_time', 'zoo_aset_sync'),\n",
    "('ext_time', 'zkHandleInit'),\n",
    "('ext_time', 'zkHandleClose'),\n",
    "('ext_time', 'znode_get_array'),\n",
    "('wait_time', 'wait_PlanCacheHash_lock'),\n",
    "('cpu_time', 'locationcache_invalidate_key'),\n",
    "('cpu_time', 'locationcache_invalidate_full'),\n",
    "('cpu_time', 'locationcache_invalidate_era'),\n",
    "('cpu_time', 'bufmgr_getfreebuf'),\n",
    "('wait_time', 'wait_ProcArray_lock'),\n",
    "('wait_time', 'wait_SinvalRead_lock'),\n",
    "('wait_time', 'wait_SinvalWrite_lock'),\n",
    "('wait_time', 'wait_LockTable_lock'),\n",
    "('wait_time', 'wait_Backend_lock'),\n",
    "('wait_time', 'wait_PlanCacheLRU_lock'),\n",
    "('wait_time', 'wait_DdlVersionLock'),\n",
    "('cpu_time', 'ostream_add_record'),\n",
    "('io_time', 'ostream_open_extent'),\n",
    "('io_time', 'ostream_close_extent'),\n",
    "('io_time', 'ostream_persist_dset'),\n",
    "('cpu_time', 'lsm_ew_add_records'),\n",
    "('cpu_time', 'lsm_ew_add_to_extent'),\n",
    "('cpu_time', 'plancache_func_callback'),\n",
    "('cpu_time', 'plancache_rel_callback'),\n",
    "('cpu_time', 'plancache_reset'),\n",
    "('cpu_time', 'locationcache_inval_era_single'),\n",
    "('cpu_time', 'locationcache_inval_era_bulk'),\n",
    "('wait_time', 'wait_prefetchQ_lock'),\n",
    "('cpu_time', 'locationcache_inval_from_bitmap'),\n",
    "('cpu_time', 'funccache_inval_local'),\n",
    "('cpu_time', 'funccache_insert'),\n",
    "('cpu_time', 'funccache_delete'),\n",
    "('cpu_time', 'funccache_lookup'),\n",
    "('cpu_time', 'plancache_getspace'),\n",
    "('cpu_time', 'plancache_getplan'),\n",
    "('cpu_time', 'plancache_onetimeplan'),\n",
    "('cpu_time', 'plancache_cachedplan'),\n",
    "('cpu_time', 'extentmerge_scan'),\n",
    "('cpu_time', 'daemon_unattributed'),\n",
    "('wait_time', 'wait_FuncCacheHash_lock'),\n",
    "('wait_time', 'wait_FuncCacheLRU_lock'),\n",
    "('cpu_time', 'funccache_drain'),\n",
    "('cpu_time', 'funccache_getspace'),\n",
    "('cpu_time', 'funccache_inval_global'),\n",
    "('cpu_time', 'funccache_evict'),\n",
    "('cpu_time', 'funccache_lruadd'),\n",
    "('cpu_time', 'funccache_recycle'),\n",
    "('cpu_time', 'funccache_publish'),\n",
    "('cpu_time', 'funccache_free'),\n",
    "('cpu_time', 'funccache_copy'),\n",
    "('cpu_time', 'funccache_copy_spi'),\n",
    "('cpu_time', 'funccache_copy_fail'),\n",
    "('cpu_time', 'funccache_copy_spi_fail'),\n",
    "('cpu_time', 'pscan_fetch'),\n",
    "('cpu_time', 'bloom_check'),\n",
    "('cpu_time', 'bloom_add'),\n",
    "('wait_app_time', 'dbms_alert_wait'),\n",
    "('wait_app_time', 'sfdc_ipc_wait'),\n",
    "('wait_app_time', 'pg_sleep'),\n",
    "('cpu_time', 'intervalStats_work'),\n",
    "('cpu_time', 'intervalStats_rollup'),\n",
    "('cpu_time', 'intervalStats_package'),\n",
    "('wait_time', 'intervalStats_wait_retry'),\n",
    "('wait_time', 'wait_catsrv_lock'),\n",
    "('io_time', 'catsrv_read'),\n",
    "('io_time', 'catsrv_write'),\n",
    "('cpu_time', 'spl_exec_check_plan'),\n",
    "('cpu_time', 'spl_exec_prepare_plan'),\n",
    "('wait_time', 'wait_FuncCacheSPI_lock'),\n",
    "('cpu_time', 'spl_expr_lock_check_plan'),\n",
    "('cpu_time', 'spl_expr_lock_publish_plan'),\n",
    "('cpu_time', 'compress'),\n",
    "('cpu_time', 'decompress')\n",
    ";\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS et_tmp_bes_17 AS\n",
    "SELECT \n",
    "    trace_record_id,\n",
    "    hostname,\n",
    "    from_unixtime(CAST(event_details['interval_start'] as bigint) DIV CAST(power(10, 9) AS bigint)) AS interval_start,\n",
    "    rectype,\n",
    "    per_proc_stats.end2end_time_stats as e2eMap\n",
    "FROM trace_avro_v3\n",
    "WHERE format_mode = 'EVENT_STATS'\n",
    " AND instr(rectype, 'Interval Metrics') > 0\n",
    "'''\n",
    "    hql += \"AND file_dt >= '{minDate}'  and file_dt <= '{maxDate}'\".format(minDate=minDate,\n",
    "                                                                             maxDate=maxDate)\n",
    "    hql += 'AND ('\n",
    "    for hi, hn in enumerate(hostnames):\n",
    "        hql += \"    (concat(split(label, '-')[0], '-', split(label, '-')[3]) =  '\" + hn + \"' )\"\n",
    "        if hi+1 < len(hostnames):\n",
    "            hql += '\\n   OR '\n",
    "            \n",
    "    hql += '''\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS et_tmp_e2e_time AS\n",
    "SELECT \n",
    "    a.trace_record_id,\n",
    "    a.hostname,\n",
    "    a.interval_start,\n",
    "    a.rectype,\n",
    "    IF(a.proc_process_key == 'recvCount', a.proc_process_value, NULL) AS recvCount,\n",
    "    a.proc_process_key,\n",
    "    a.proc_process_value\n",
    "    FROM (\n",
    "        SELECT \n",
    "            trace_record_id,\n",
    "            hostname,\n",
    "            interval_start,\n",
    "            rectype,\n",
    "            mk AS proc_process_key,\n",
    "            mv AS proc_process_value\n",
    "          FROM et_tmp_bes_17\n",
    "          LATERAL VIEW explode(e2eMap) m AS mk, mv) a\n",
    "    WHERE instr(a.proc_process_key, 'Count') == 0\n",
    "       OR a.proc_process_key == 'recvCount';\n",
    "    \n",
    "CREATE TABLE IF NOT EXISTS et_tmp_e2e_catMap AS\n",
    "SELECT \n",
    "    l.*,\n",
    "    r.categoryName,\n",
    "    r.metricName\n",
    "  FROM et_tmp_e2e_time as l\n",
    "  JOIN et_timerMap as r\n",
    "  ON l.proc_process_key == r.metricName;\n",
    "  \n",
    "  \n",
    "CREATE TABLE IF NOT EXISTS et_tmp_e2e_catMapSum AS\n",
    "SELECT \n",
    "    hostname,\n",
    "    trace_record_id,\n",
    "    interval_start,\n",
    "    rectype,\n",
    "    categoryName,\n",
    "    SUM(proc_process_value) as sumTime\n",
    "FROM et_tmp_e2e_catMap\n",
    "GROUP BY hostname, trace_record_id, interval_start, rectype, categoryName;\n",
    "\n",
    "set hive.cli.print.header=true;\n",
    "SELECT \n",
    "    l.hostname AS hostname,\n",
    "    l.trace_record_id AS trace_record_id,\n",
    "    l.interval_start AS interval_start,\n",
    "    substr(l.rectype, length('Interval Metrics ')+1, length(l.rectype)) AS process_type,\n",
    "    l.categoryName AS cpu_category_name,\n",
    "    l.sumTime AS sum_time_ns,\n",
    "    r.recvCount AS recvCount\n",
    "FROM et_tmp_e2e_catMapSum as l\n",
    "LEFT OUTER JOIN (\n",
    "    SELECT trace_record_id,\n",
    "        recvCount\n",
    "    FROM et_tmp_e2e_time\n",
    "    WHERE recvCount is not NULL\n",
    ") r\n",
    "ON l.trace_record_id = r.trace_record_id\n",
    "WHERE l.categoryName == 'cpu_time';\n",
    "\n",
    "DROP TABLE IF EXISTS et_timerMap;\n",
    "DROP TABLE IF EXISTS et_tmp_bes_17;\n",
    "DROP TABLE IF EXISTS et_tmp_e2e_time;\n",
    "DROP TABLE IF EXISTS et_tmp_e2e_catMap;\n",
    "DROP TABLE IF EXISTS et_tmp_e2e_catMapSum;\n",
    "    '''\n",
    "    \n",
    "    return(hql)\n",
    "\n",
    "def transposeGroupedByDataFrame(df,\n",
    "                                groupby=[],\n",
    "                                splitby=None,\n",
    "                                fieldNameToSplit=None):\n",
    "    metaFieldNames = groupby\n",
    "    hierachicalIndexValues = []\n",
    "    for fn in metaFieldNames:\n",
    "        hierachicalIndexValues.append(df[fn].tolist())\n",
    "\n",
    "    # add field containing field names to be use for new fields\n",
    "    hierachicalIndexValues.append(df[splitby].tolist())\n",
    "\n",
    "    indexNames = copy.copy(metaFieldNames)\n",
    "    indexNames.append(splitby)\n",
    "    index = pd.MultiIndex.from_arrays(hierachicalIndexValues, names=indexNames)\n",
    "\n",
    "    tempDf = pd.DataFrame(df[fieldNameToSplit])\n",
    "    tempDf.set_index(index, inplace=True)\n",
    "    tDf = tempDf.unstack(level=splitby,\n",
    "                         fill_value=0.0)\n",
    "    tDf.reset_index(level=range(len(metaFieldNames)), inplace=True)\n",
    "    tDf = pd.DataFrame(tDf.to_records(index=False))\n",
    "    \n",
    "    # need to \"clean up\" resulting field names\n",
    "    renameDict = {}\n",
    "    renameRe = re.compile('\\(\\'([^\\']*)\\',\\s*\\'([^\\']*)\\'\\)')\n",
    "    for ln in tDf.columns.values:\n",
    "\n",
    "        lnMatch = renameRe.match(ln)\n",
    "        if lnMatch is not None:\n",
    "            indexName = lnMatch.group(1)\n",
    "            valueName = lnMatch.group(2)\n",
    "            if valueName == '':\n",
    "                renameDict[ln] = indexName\n",
    "            else:\n",
    "                renameDict[ln] = valueName\n",
    "\n",
    "    tDf.rename(columns=renameDict, inplace=True)\n",
    "    \n",
    "    return(tDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TTransportException",
     "evalue": "Could not connect to any of [('10.9.103.79', 10000)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTTransportException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-11c04cb7996f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmyConn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetHiveConn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mmyConn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-11c04cb7996f>\u001b[0m in \u001b[0;36mgetHiveConn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetHiveConn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msdbKP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fliang-wsl3.internal.salesforce.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msdbKP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fliang\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pyhive/hive.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, port, username, database, auth, configuration, kerberos_service_name, password, thrift_transport)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             open_session_req = ttypes.TOpenSessionReq(\n\u001b[1;32m    194\u001b[0m                 \u001b[0mclient_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/thrift_sasl/__init__.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msasl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/thrift/transport/TSocket.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m                                                           addrs))\n\u001b[1;32m    112\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTTransportException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTTransportException\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_OPEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTTransportException\u001b[0m: Could not connect to any of [('10.9.103.79', 10000)]"
     ]
    }
   ],
   "source": [
    "def getHiveConn():\n",
    "    sdbKP = 'fliang-wsl3.internal.salesforce.com'\n",
    "    conn = hive.Connection(host=sdbKP, port=10000, username=\"fliang\")\n",
    "    return(conn)\n",
    "\n",
    "myConn = getHiveConn()\n",
    "print myConn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TTransportException",
     "evalue": "Could not connect to any of [('10.253.212.74', 10000)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTTransportException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6df043b05bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# connect and get data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmyConn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetHiveConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mmyConn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0minDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyConn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ab5a67808ee5>\u001b[0m in \u001b[0;36mgetHiveConnection\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msdbKP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sayonara1a-mnds1-2-prd.eng.sfdc.net'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     conn = hive.connect(sdbKP, \n\u001b[0;32m---> 17\u001b[0;31m                         configuration={'hive.fetch.task.conversion': 'none'})\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m def createHql(minDate=None,\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pyhive/hive.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mConnection\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pyhive/hive.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, port, username, database, auth, configuration, kerberos_service_name, password, thrift_transport)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             open_session_req = ttypes.TOpenSessionReq(\n\u001b[1;32m    194\u001b[0m                 \u001b[0mclient_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/thrift_sasl/__init__.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msasl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/thrift/transport/TSocket.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m                                                           addrs))\n\u001b[1;32m    112\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTTransportException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTTransportException\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_OPEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTTransportException\u001b[0m: Could not connect to any of [('10.253.212.74', 10000)]"
     ]
    }
   ],
   "source": [
    "# hive query isn't currently working from the jupyter notebook at this time (9/17/2018)\n",
    "inputFilename = None #'2018-09-10.cpuBuckets.tsv' # = None if want to fetch from hive\n",
    "noisy = True\n",
    "numCores = {'gs1-phx.ops.sfdc.net': 40,\n",
    "            'cs999-dfw.ops.sfdc.net': 40 # will = 80 after 9/15 when hyper threading was enabled\n",
    "           }\n",
    "\n",
    "# run query or read file\n",
    "inDf = None \n",
    "graphicMinTime =  None # dt.datetime.strptime('09/13/2018 13:18:49', '%m/%d/%Y %H:%M:%S')\n",
    "graphicMaxTime =  None # dt.datetime.strptime('09/13/2018 15:10:32', '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "if inputFilename is None:\n",
    "    minDate = '2018-09-14' # these only matter when running hive query from jupyter notebook; else uses file contents\n",
    "    maxDate = '2018-09-14'\n",
    "    hql = createHql(minDate = minDate, \n",
    "                    maxDate = maxDate, \n",
    "                    hostnames = ['cs999-dfw.ops.sfdc.net', 'gs1-phx.ops.sfdc.net'])\n",
    "    # write the query to disk in case we need to execute in kpclient\n",
    "    fp = open('cpuBuckets.sql', 'w')\n",
    "    fp.write(hql)\n",
    "    fp.close()\n",
    "    \n",
    "    # connect and get data\n",
    "    myConn = getHiveConnection()\n",
    "    print myConn\n",
    "    inDf = pd.read_sql(hql, myConn)\n",
    "    \n",
    "    dataFilename = '{0}_{1}.cpuRawData.tsv'.format(minDate, maxDate)\n",
    "    inDf.to_csv(dataFilename, sep='\\t', index=False)\n",
    "else:\n",
    "    inDf = pd.read_csv(inputFilename, sep='\\t')\n",
    "if noisy:\n",
    "    print inDf.columns.values\n",
    "    \n",
    "# split cpu rows into columns and create a distinct df for each hostname\n",
    "df = inDf.loc[inDf['cpu_category_name'] == 'cpu_time', :]\n",
    "allDaemon = np.unique(df['process_type'])\n",
    "logicalHostnames = []\n",
    "for r in df['hostname'].values:\n",
    "    # create logical hostname\n",
    "    lhnParts = r.split('-')\n",
    "    lhn = '-'.join([lhnParts[0], lhnParts[3]])\n",
    "    logicalHostnames.append(lhn)\n",
    "\n",
    "df['logicalHostname'] = logicalHostnames\n",
    "\n",
    "\n",
    "for lhn in np.unique(df['logicalHostname']):\n",
    "    hdf = df.loc[df['logicalHostname'] == lhn]\n",
    "    procNames = np.unique(hdf['process_type'])\n",
    "    tt = dt.datetime.strptime(np.min(hdf['interval_start'].values), '%Y-%m-%d %H:%M:%S')\n",
    "    minDateStr = dt.datetime.strftime(tt, '%Y-%m-%d')\n",
    "    \n",
    "    # determine master at each minute\n",
    "    rcdf = hdf.loc[hdf['process_type'] == 'Backend processes']\n",
    "    hostnames = np.unique(rcdf['hostname'])\n",
    "    recvCountDf = transposeGroupedByDataFrame(rcdf,\n",
    "                                              groupby=['logicalHostname', 'interval_start'],\n",
    "                                              splitby='hostname',\n",
    "                                              fieldNameToSplit='recvcount')\n",
    "    recvCountDf['maxRecvCount'] = recvCountDf[hostnames].max(axis=1)\n",
    "    recvCountDf['maxAtIndex'] = np.argmax(np.array(recvCountDf[hostnames]), axis=1)\n",
    "    \n",
    "    # find values of interest from the master node for each time slice\n",
    "    final = {'interval_start': [], \n",
    "             'master_nodename': []}\n",
    "    for pn in procNames:\n",
    "        final[pn] = []\n",
    "    for i, t in enumerate(recvCountDf['interval_start']):\n",
    "        if graphicMinTime is not None:\n",
    "            tt = dt.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "            if tt < graphicMinTime :\n",
    "                continue\n",
    "        if graphicMaxTime is not None:\n",
    "            tt = dt.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "            if tt > graphicMaxTime :\n",
    "                continue\n",
    "                \n",
    "        mn = hostnames[recvCountDf['maxAtIndex'][i]]\n",
    "        for pn in procNames:\n",
    "            sumtime_ns = hdf.loc[(hdf['hostname'] == mn) &\n",
    "                                 (hdf['interval_start'] == t) & \n",
    "                                 (hdf['process_type'] == pn), 'sum_time_ns'].values\n",
    "            if len(sumtime_ns) == 0:\n",
    "                sumtime_ns = 0\n",
    "            elif len(sumtime_ns) == 1:\n",
    "                sumtime_ns = sumtime_ns[0]\n",
    "            else:\n",
    "                print 'WARNING: Found more than one process time value for 1 time x 1 process_name'\n",
    "            final[pn].append(sumtime_ns)\n",
    "        final['interval_start'].append(t)\n",
    "        final['master_nodename'].append(mn)\n",
    "        if i % 10 == 0:\n",
    "            if noisy:\n",
    "                print i, t\n",
    "        #if i >= 1440:\n",
    "        #    break\n",
    "\n",
    "    # convert to df and add percent cpu of all available time / min    \n",
    "    finalDf = pd.DataFrame(final)\n",
    "    \n",
    "    colsToPlot = []\n",
    "    yList = []\n",
    "    yStatForOrderingLegend = {}\n",
    "    for pn in procNames:\n",
    "        pcpu = '%CPU[{0}]'.format(pn)\n",
    "        finalDf[pcpu] = 100.0 * finalDf[pn]/(numCores[lhn] * 60.0 * 10.0**9.0) # convert ns / min to %of all available cores/min\n",
    "        yStatForOrderingLegend[pn] = np.percentile(finalDf[pcpu], 50.0)\n",
    "    finalDataFilename = '{0}.{1}.cpuPerDaemon_data.tsv'.format(minDateStr, lhn)\n",
    "    finalDf.to_csv(finalDataFilename, sep='\\t', index=False)\n",
    "    \n",
    "    # add to Y in decreasing order of average %CPU\n",
    "    legendKey = []\n",
    "    sortedProcNames = sorted(yStatForOrderingLegend.items(), key=itemgetter(1), reverse=True)\n",
    "    for pn,pc in sortedProcNames:\n",
    "        pcpu = '%CPU[{0}]'.format(pn)\n",
    "        yList.append(finalDf[pcpu].values)\n",
    "        legendKey.append(pn)\n",
    "    # create graphics\n",
    "    x = finalDf['interval_start']\n",
    "    numX = len(x)\n",
    "    y = np.vstack(yList)\n",
    "    \n",
    "    imageWidthInches = int(float(numX)/20.0)\n",
    "    if imageWidthInches > 2.0**16:\n",
    "        imageWidthInches = 2.0**16 - 1\n",
    "    fig = plt.figure(figsize=(imageWidthInches,10))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.stackplot(x, y, labels=legendKey)\n",
    "    plt.grid(True, 'major', 'y', ls='--', lw=.5, c='k', alpha=.3)\n",
    "    plt.grid(True, 'major', 'x', ls='--', lw=.5, c='k', alpha=.3)\n",
    "    plt.xticks(range(0,numX,10), x[0:numX:10], rotation=90)\n",
    "    minY = np.min(y)\n",
    "    maxY = np.max(y)\n",
    "    if maxY > 600:\n",
    "        percentileLevelY = np.percentile(y, 99.99)\n",
    "        if percentileLevelY < 100.0:\n",
    "            plt.ylim(ymax=100.0)\n",
    "        elif percentileLevelY > 600.0:\n",
    "            three9s = np.percentile(y, 99.9)\n",
    "            if three9s < 600.0:\n",
    "                plt.ylim(ymax=three9s)\n",
    "            else:\n",
    "                plt.ylim(ymax=600.0)\n",
    "        else:\n",
    "            plt.ylim(ymax=percentileLevelY)\n",
    "            \n",
    "    if minY < 0:\n",
    "        plt.ylim(ymin=0)\n",
    "    plt.title('Percent CPU: {0}'.format(lhn))\n",
    "    print lhn, np.min(x), np.max(x)\n",
    "    #plt.xlim(xmin=np.min(x), xmax=np.max(x))\n",
    "    \n",
    "    # Shrink axis to make room for legend and x-axis labels\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height * 0.85])\n",
    "\n",
    "    # Put a legend to the right of the current axis\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # add text indicating the number of cores used to convert to percentage values\n",
    "    #plt.text(np.min(x), 0.90*np.max(y), '#cores={0}'.format(numCores[lhn]), fontsize=10)\n",
    "    \n",
    "    imFilename = '{0}.{1}.cpuPerDaemon.jpg'.format(minDateStr, lhn)\n",
    "    plt.savefig(imFilename, dpi=100)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
