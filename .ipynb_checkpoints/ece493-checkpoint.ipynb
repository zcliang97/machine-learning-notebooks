{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE493: Probabilistic Reasoning and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "* Introduction\n",
    "* Probability Review\n",
    "\n",
    "## Representation\n",
    "\n",
    "* Bayesian Networks (Directed Graphs)\n",
    "* Markov Random Fields (Undirected Graphs)\n",
    "\n",
    "## Inference\n",
    "\n",
    "* Variable Elimination\n",
    "* MAP Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Probablistic graphical modeling** is a branch of machine learning that studies how to use probability distributions to describe the world and make useful predictions about it.\n",
    "\n",
    "The simplest model would be a linear equation of the form:\n",
    "\n",
    "$y = \\beta^Tx$\n",
    "\n",
    "* y is the outcome variable that we want to predict\n",
    "* x is the series of factors that affect the outcome\n",
    "* $\\beta$ is the parameters\n",
    "\n",
    "However, the real world has _uncertainty_ so this is usually dealt with a probability distribution.\n",
    "\n",
    "$p(x, y)$\n",
    "\n",
    "## Difficulties of Probabilistic Modeling\n",
    "\n",
    "Suppose, we have a binary classifier that determines if an email is spam. This is done through a large list of words and determining if these words appear in an email will ultimately determine if the email is spam. However, if this list is very large, then we would have to write down all of these values.\n",
    "\n",
    "This process can be simplified through _conditional independence_ among the variables. This process is called the **Naive Bayes** assumption. Given this assumption, we can model the probabilities as a product of factors:\n",
    "\n",
    "$P(y, x_1, x_2, ..., x_n) = p(y)\\sum_{i=1}^np(x_i|y)$\n",
    "\n",
    "In this case, each factor $p(x_i|y)$ can be completely described by a small number of parameters (4 parameters with 2 degrees of freedom). This entire distribution is parametrized by $O(n)$ parameters which we can tractably estimate from data and make predictions.\n",
    "\n",
    "## Graphical Representation\n",
    "<img src=\"images/naive_bayes.png\" width=\"50%\">\n",
    "\n",
    "## Overview of the Course\n",
    "### Representation\n",
    "This is how to specify a model.\n",
    "\n",
    "This is a difficult process with lots of input parameters. Will use a lot of graph theory.\n",
    "\n",
    "### Inference\n",
    "Given a model, how do we extract useful information. There are two kinds of inference:\n",
    "* _Marginal Inference_: what is the probability of a given variable in our model after we sum everything else out? An example query would be to determine the probability that a random house has more than three bedrooms.\n",
    "\n",
    "$p(x_1) = \\sum_{x_2}\\sum_{x_3}...\\sum_{x_n}p(x_1, x_2, ..., x_n)$\n",
    "\n",
    "* Maximum a posteriori (MAP) inference: asks for the most likely assignment of variables. For example, we may try to determine the most likely spam message, solving the problem.\n",
    "\n",
    "$max_{x_1, ..., x_n} p(x_1, ..., x_n, y = 1)$\n",
    "\n",
    "### Learning\n",
    "\n",
    "Refers to fitting a model to a dataset, which could be for example a large number of labeled examples of spam. By looking at the data, we can infer useful patterns (eg. which wordsare found more frequently in spam emails), which we can then use to make predictions about the future. However, we will see that learning and inference are also inherently linked in a more subtle way since inference will turn out to be a key subroutine that we will repeatedly call within learning algorithms. Also, the tpoic of learning will feature important connection to the field of computational learning theory - which deals with questions such as generalization from limited data and overfitting, as well as to Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Review\n",
    "\n",
    "## 1. Elements of Probability\n",
    "\n",
    "**Sample Space $\\Omega$:** the set of all outcomes of a random experiment. Here, each outcome $\\omega\\in\\Omega$ can be thought of as a complete description of the state of the real world at the end of the experiment.\n",
    "\n",
    "**Set of Events (or event space) F:** A set whose elements $A\\in F$ (called events) are subsets of $\\Omega$ (ie. $A\\subset\\Omega$ is a collection of possible outcomes of an experiment).\n",
    "\n",
    "**Probability Measure:** A function $P:F\\implies\\mathbb{R}$ that satisfies the following properties\n",
    "* $P(A)\\geq0$ for all $A\\subset F$\n",
    "* If $A_1, A_2, ...$ are disjoint events (ie. $A_i\\cap A_j = \\emptyset$ whenever $i\\neq j$), then $P(\\cup_iA_i) = \\sum_iP(A_i)$\n",
    "* $P(\\Omega) = 1$\n",
    "\n",
    "These three properties are called the *Axioms of Probability*.\n",
    "\n",
    "**Example:** Consider the event of tossing a six-sided die. The sample space is $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$. We can define different event spaces on this sample space. For example, the simplest event space is the trivial event space $F = {\\emptyset, \\Omega}$. Another event space is the set of all subsets of $\\Omega$.\n",
    "\n",
    "For the first event space, the unique probability measure satisfying the requirements above is given by P($\\emptyset$) = 0, P($\\Omega$) = 1. For the second event space, one valid measure is to assign the probability of each set in the event space to be $\\frac {i}{6}$. For example, $P(\\{1, 2, 3, 4\\}) = \\frac{4}{6}$.\n",
    "\n",
    "### _Properties_\n",
    "* $A \\in B \\implies P(A) \\leq P(B)$\n",
    "* $P(A\\cap B) \\leq min(P(A), P(B))$\n",
    "* Union Bound: $P(A\\cup B) \\leq P(A) + P(B)$\n",
    "* $P(\\Omega - A): 1-P(A)$\n",
    "* Law of Total Probability: If $A_1, ..., A_k$ are a set of disjoint events such that $\\cup_{i=1}^k A_i = \\Omega$, then $\\sum_{i=1}^k P(A_i) = 1$\n",
    "\n",
    "### 1.1 Conditional Probability\n",
    "Let B be an event with non-zero probability. The donctional probablity of any event A given B is:\n",
    "\n",
    "$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "### 1.2 Chain Rule\n",
    "Let $S_1, ..., S_k$ be events, $P(S_i) > 0$. Then the chain rule states that:\n",
    "\n",
    "$P(S_1\\cap S_2\\cap...\\cap S_k) = P(S_1)P(S_2|S_1)P(S_3|S_2\\cap S_1)...P(S_k|S_1\\cap S_2\\cap...\\cap S_{k-1})$ \n",
    "\n",
    "Note that for $k=2$ events, this is just the definition of conditional probability:\n",
    "\n",
    "$P(S_1\\cap S_2) = P(S_1)P(S_2|S_1)$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$P(S_1\\cap S_2 \\cap S_3 \\cap S_4)$\n",
    "\n",
    "$= P(S_1 \\cap S_2 \\cap S_3)P(S_4 | S_1 \\cap S_2 \\cap S_3)$\n",
    "\n",
    "$= P(S_1 \\cap S_2)P(S_3 | S_1 \\cap S_2)P(S_4 | S_1 \\cap S_2 \\cap S_3)$\n",
    "\n",
    "$= P(S_1)P(S_2|S_1)P(S_3 | S_1 \\cap S_2)P(S_4 | S_1 \\cap S_2 \\cap S_3)$\n",
    "\n",
    "### 1.3 Indepedence\n",
    "\n",
    "Two events are called **independent** if $P(A\\cap B) = P(A)P(B)$ or $P(A|B) = P(A)$. Intuitively, A and B are independent means that observing B does not have any effect on the probability of A.\n",
    "\n",
    "## 2. Random Variables\n",
    "\n",
    "Consider an experiment where we flip 5 coins and we want to know the number of heads. here the elements of the sample saplce $\\Omega$ are 5-length sequences of heads and tails. In practice, we care about real-valued functions of outcomes, such as the number of heads that apear among 5 tosses or the length of the longest run of tails. These functions under some techincal conditions are known as **random variables**.\n",
    "\n",
    "More formally, a random variable **X** is a function $X:\\Omega\\implies\\mathbb{R}$. Typically, we denote random variables using upper case letters $X(\\omega)$ or simply $X$ (where the dependence on the random outcome $\\omega$ is implied). We denote the value that a random variable may take on using lower case letters $x$. Thus, $X = x$ means that we assign . the value $x\\in\\mathbb{R}$ to the random variable $X$.\n",
    "\n",
    "**Example:** In the experiment above, suppose that $X(\\omega)$ is the number of heads which occur in the sequence of tosses $\\omega$. Given that only 5 coins are tossed, $X(\\omega)$ can take only a finite number of values, so it is known as a **discrete random variable**. Here the probability of the set associated with a random variable $X$ taking on some specific value $k$ is:\n",
    "\n",
    "$P(X=k) := P({w:X(\\omega) = k})$\n",
    "\n",
    "**Example:** Suppose that $X(\\omega)$ is a random variable indicating the amount of time it takes for a radioactive particle to deay. In this case, $X(\\omega)$ takes on an infinite nubmer of possible values, so it is called a **continuous random variable**. We denote the probability that $X$ takes on a value between two real constants $a$ and $b$ (where $a<b$) as:\n",
    "\n",
    "$P(a \\leq X \\leq b) := P({w: a \\leq X(\\omega) \\leq b})$\n",
    "\n",
    "----\n",
    "\n",
    "To specify the probability lmeasures used when dealing with random variables, it is often convenient to specify alternative functions (CDFs, PDFs, and PMFs) from which the probability measure governing an experiment immediately follows. \n",
    "\n",
    "### 2.1 Cumulative Distribution Functions\n",
    "\n",
    "<img src=\"images/cdf.png\" width=\"50%\"/>\n",
    "\n",
    "A Cumulative Distribution Function (CDF) is a function $F_X : \\mathbb{R}\\implies[0, 1]$ which specifies a probability measure as:\n",
    "\n",
    "$F_X(x) = P(X\\leq x)$.\n",
    "\n",
    "By using this function, one can calculate the probability of any event.\n",
    "\n",
    "#### _Properties:_\n",
    "* $0\\leq F_X(x) \\leq 1$\n",
    "* $\\lim_{x\\to-\\infty} F_X(x) = 0$\n",
    "* $\\lim_{x\\to+\\infty} F_X(x) = 1$\n",
    "* $x \\leq y \\implies F_X(x) \\leq F_X(y)$\n",
    "\n",
    "### 2.2 Probability Mass Functions\n",
    "\n",
    "<img src=\"images/pmf.png\" width=\"50%\"/>\n",
    "\n",
    "When a random variable $X$ takes on a finite set of possible values (ie. $X$ is a discrete random variable), a simpler way to represent the probability measure associated with a random variable is to directly specify the probability of each value that the random variable can assume. In particular, a probability mass function (PMF) is a function $p_X : \\Omega\\implies\\mathbb{R}$ such that $p_X(x) = P(X=x)$.\n",
    "\n",
    "In the case of discrete random variable, we use the notation $Val(X)$ for the set of possible values that the random variable $X$ may assume. For example, if $X(\\omega)$ is a random variable indicating the number of heads out of 5 coin tosses, then $Val(X) = {0, 1, 2, 3, 4, 5}$.\n",
    "\n",
    "#### _Properties:_\n",
    "* $0 \\leq p_X(x) \\leq 1$\n",
    "* $\\sum_{x\\in Val(X)}p_X(x) = 1$\n",
    "* $\\sum_{x\\in A}p_X(x) = P(X\\in A)$\n",
    "\n",
    "### 2.3 Probability Density Functions\n",
    "\n",
    "<img src=\"images/pdf.png\" width=\"50%\"/>\n",
    "\n",
    "For some continuous random variables, the cumulative distribution function $F_X(x)$ is differentiable everywhere. In these cases, we define the Probability Density Function (PDF) as the derivative of the CDF, ie.\n",
    "\n",
    "$f_X(x) = \\frac{dF_X(x)}{dx}$\n",
    "\n",
    "Note that the PDF for a continuous random variable may not always exist (ie. If F_X(x) is not differentiable everywhere).\n",
    "\n",
    "According to the properties of differentiation, for every small $\\delta x$:\n",
    "\n",
    "$P(x \\leq X \\leq x + \\delta x) \\approx f_X(x)\\delta x$\n",
    "\n",
    "Both CDFs and PDFs (when they exist) can be used for calculating the probabilities of different events. But it should be emphasized that the value of PDF at any given point $x$ is not the probability of that event, i.e, $f_X(x)\\neq P(X=x)$. For example, $f_X(x)$ can take on values larger than one (but the integral of $f_X(x)$ over any subset of $\\mathbb{R}$ will be at most one).\n",
    "\n",
    "#### _Properties:_\n",
    "* $f_X(x) \\geq 0$\n",
    "* $\\int_{-\\infty}^{\\infty}f_X(x) = 1$\n",
    "* $\\int_{x\\in A}f_X(x)dx = P(X\\in A)$\n",
    "\n",
    "### 2.4 Expectation\n",
    "\n",
    "The **expectation** of a function $g(X)$ can be thought of an a \"weighted average\" of the values that $g(X)$ can be taken for different values of $x$. Suppose that $X$ is a discrete random variable with PMF $p_X(x)$ and $g: \\mathbb{R}\\to\\mathbb{R}$ is an arbitrary function. In this case, $g(X)$ can be considered a random variable, and we define the **expectation** or expected value of $g(X)$ as:\n",
    "\n",
    "$\\mathbb{E}[g(X)] = \\sum_{x\\in Val(x)} g(x)p_X(x)$\n",
    "\n",
    "If $X$ is a continuous random variable with PDF $f_X(x)$, then the expected value of $g(X)$ is defined as:\n",
    "\n",
    "$\\mathbb{E}[g(X)] = \\int_{-\\infty}^{+\\infty} g(x)f_X(x)$\n",
    "\n",
    "#### _Properties:_\n",
    "* $\\mathbb{E}[a] = a$ for any constant $a\\in\\mathbb{R}$\n",
    "* $\\mathbb{E}[af(X)] = a\\mathbb{E}[f(x)]$ for any constant $a\\in\\mathbb{R}$\n",
    "* (Linearity of Expectation): $\\mathbb{E}[f(X) + g(X)] = \\mathbb{E}[f(X)] + \\mathbb{E}[g(X)]$\n",
    "* For a discrete random variable $X, \\mathbb{E}[1\\{X=k\\}] = P(X=k)$\n",
    "\n",
    "### 2.5 Variance\n",
    "\n",
    "The variance of a random variable $X$ is a measure of how concentrated the distribution of a random variable $X$ is around its mean. Formally, the variance of a random variable $X$ is defined as $Var[X] = \\mathbb{E}[(X-\\mathbb{E}[X])^2]$.\n",
    "\n",
    "Using properties in the previous section, we can derive an alternate expression for the variance:\n",
    "\n",
    "$\\mathbb{E}[(X-\\mathbb{E}[x])^2]$\n",
    "\n",
    "$= \\mathbb{E}[X^2-2\\mathbb{E}[X]X+\\mathbb{E}[X]^2]$\n",
    "\n",
    "$= \\mathbb{E}[X^2] - 2\\mathbb{E}[X]\\mathbb{E}[X]+\\mathbb{E}[X]^2$\n",
    "\n",
    "$= \\mathbb{E}[X^2]-\\mathbb{E}[X]^2$\n",
    "\n",
    "#### _Properties:_\n",
    "* $Var[a] = 0$ for any constant $a\\in \\mathbb{R}$\n",
    "* $Var[af(X)] = a^2Var[f(X)]$ for any constant $a\\in \\mathbb{R}$\n",
    "\n",
    "### 2.6 Some Common Random Variables\n",
    "\n",
    "#### Discrete Random Variables\n",
    "* $X\\sim Bernoulli(p)$ (where $0 \\leq p \\leq 1)$: the outcome of a coin flip (H = 1, T= 0) for a coin that comes up heads with probability p.\n",
    "\n",
    "$p(x)= \\begin{cases} \n",
    "          p, & if x=1 \\\\\n",
    "          1-p, & if x=0\n",
    "       \\end{cases}$\n",
    "       \n",
    "* $X\\sim Binomial(n, p)$ (where $0 \\leq p \\leq 1$): the number of heads in $n$ independent flips of a coin with heads probability $p$.\n",
    "\n",
    "$p(x)= {n\\choose x}p^x(1-p)^{n-x}$\n",
    "\n",
    "* $X\\sim Geometric(p)$ (where $p > 0$): the number of flips of a coin until the first heads, for a coin that comes up heads with probability $p$.\n",
    "\n",
    "$p(x) = p(1-p)^{x-1}$\n",
    "\n",
    "* $X\\sim Poisson(\\lambda)$ (where $\\lambda > 0$): a probability distribution over the non-negative integers used for modeling the frequency of rare events.\n",
    "\n",
    "$p(x) = e^{-\\lambda}\\frac{\\lambda^x}{x!}$\n",
    "\n",
    "#### Continuous Random Variables\n",
    "* $X\\sim Uniform(a, b)$ (where $a < b$): equal probability density to every value between a and b on the real line.\n",
    "\n",
    "$f(x) = \\begin{cases} \n",
    "          \\frac{1}{b-a}, & if a \\leq b \\\\\n",
    "          0, & otherwise\n",
    "       \\end{cases}$\n",
    "       \n",
    "* $X\\sim Exponential(\\lambda)$ (where $\\lambda > 0$): decaying probability density over the non-negative reals.\n",
    "\n",
    "$f(x) = \\begin{cases} \n",
    "          \\lambda e^{\\lambda x}, & if x\\geq 0 \\\\\n",
    "          0, & otherwise\n",
    "       \\end{cases}$\n",
    "       \n",
    "* $X\\sim Normal(\\mu, \\sigma^2)$: also known as the Gaussian distribution\n",
    "\n",
    "$f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "## 3. Two Random Variables\n",
    "\n",
    "There might be cases where we are interested in knowning more than one quantity during a random experiment. For example, if we flip a coin multiple times, we may care about both $X(\\omega)$ = number of heads that come up and the $Y(\\omega)$ = the length of the longest run of consecutive heads. \n",
    "\n",
    "### 3.1 Joint and Marginal Distributions\n",
    "\n",
    "Suppose that we have two random variables $X$ and $Y$. One way to work with these two random variables is to consider each of them separately. If we do that, we will only need $F_X(x) and F_Y(y)$. But if we want to know about the values that $X$ and $Y$ assume simultaneously during outcomes of a random experiment, we require a more complicated structure known as the joint cumulative distribution function of $X$ and $Y$, defined by:\n",
    "\n",
    "$F_{XY}(x, y) = P(X\\leq x, Y \\leq y)$.\n",
    "\n",
    "It can be shown that by knowning the joint cumulative distribution function, the probability of any event involving $X$ and $Y$ can be calculated.\n",
    "\n",
    "The joint CDF $F_{XY}(x, y)$ and the cumulative distribution functions $F_X(x) and F_Y(y)$ of each variable separately are related by:\n",
    "\n",
    "$F_X(x) = \\lim_{y\\to\\infty} F_{XY}(x, y)$\n",
    "\n",
    "$F_Y(x) = \\lim_{x\\to\\infty} F_{XY}(x, y)$\n",
    "\n",
    "Here, we call $F_X{x}$ and $F_Y{y}$ the **marginal cumulative distribution functions** of $F_{XY}(x, y)$.\n",
    "\n",
    "### 3.2 Joint and Marginal PMF\n",
    "\n",
    "If $X$ and $Y$ are discrete random variables, then the joint probability mass function $p_{XY}: Val(X) \\times Val(Y) \\to [0, 1]$ is defined by:\n",
    "\n",
    "$p_{XY}(x, y) = P(X=x, Y=y)$\n",
    "\n",
    "Here, $0 \\leq P_{XY}(x, y) \\leq 1$ for all x, y, and $\\sum_{x\\in Val(X)}\\sum_{y\\in Val(Y)} P_{XY}(x, y) = 1$\n",
    "\n",
    "The **marginal probability mass function** of $X$ is defined as:\n",
    "\n",
    "$p_X(x) = \\sum_yp_{XY}(x, y)$.\n",
    "\n",
    "This is also the case with $p_Y(y)$.\n",
    "\n",
    "### 3.3 Joint and Marginal PDF\n",
    "\n",
    "If $X$ and $Y$ are continuous random variables with joint distribution function $F_{XY}$. IN the case that $F_{XY}(x, y)$ is differentiable everywhere in both x and y, then the joint probability density function is:\n",
    "\n",
    "$f_{XY}(x, y) = \\frac{\\partial^2F_{XY}(x, y)}{\\partial x \\partial y}$\n",
    "\n",
    "Also:\n",
    "\n",
    "$\\int\\int_{(x,y)\\in A} f_{XY}(x, y)dxdy = P((X, Y) \\in A)$.\n",
    "\n",
    "Note the values of the probability density function $f_{XY}(x, y)$ are always non-negative but they may be greater than 1. Nonetheless, it must be the case that $\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty f_{XY}(x, y) = 1$.\n",
    "\n",
    "Analogous to the discrete case, we define **marginal probability density function** (or marginal density) of X as:\n",
    "\n",
    "$f_X(x) = \\int_{-\\infty}^\\infty f_{XY}(x,y)dy$\n",
    "\n",
    "This is also the case with $f_Y(y)$.\n",
    "\n",
    "### 3.4 Conditional Distributions\n",
    "\n",
    "What is the probability distribution over $Y$, when we know that $X$ must take on a certain value $x$?\n",
    "\n",
    "In the discrete case, the conditional PMF of $Y$ given $X$ is simply:\n",
    "\n",
    "$p_{Y|X}(y|x) = \\frac{p_{XY}(x, y)}{p_X(x)}$ \n",
    "\n",
    "assuming that $p_X(x) \\neq 0$.\n",
    "\n",
    "In the continuous case, it is more complicated as the probability that a continuous random variable $X$ takes on a specific value $x$ is equal to zero. Ignoring this technical point, we simply define the conditional PDF of $Y$ given $X = x$ as:\n",
    "\n",
    "$f_{Y|X}(y|x) = \\frac{f_{XY}(x, y}{f_X(x)}$\n",
    "\n",
    "assuming $f_X(x) \\neq 0$.\n",
    "\n",
    "### 3.5 Chain Rule\n",
    "\n",
    "Chain rule derived earlier is applicable to random variables as follows:\n",
    "\n",
    "$p_{X_1,...,X_n}(x_1, ..., x_n)$\n",
    "\n",
    "$= p_{X_1}(x_1)p_{X_2|X_1}(x_2|x_1)...p_{X_n|X_1, ..., X_{n-1}}(x_n|x_1,..., x_{n-1})$\n",
    "\n",
    "### 3.6 Baye's Rule\n",
    "\n",
    "For discrete random variables $X$ and $Y$:\n",
    "\n",
    "$P_{Y|X}(y, x) = \\frac{P_{XY}(x,y)}{P_X{(x)}} = \\frac{P_{X|Y}(x|y)P_Y(y)}{\\sum_{y'\\in Val(Y)}P_{X|Y}{(x|y')}P_Y(y')}$\n",
    "\n",
    "For continuous random variables $X$ and $Y$:\n",
    "\n",
    "$f_{Y|X}(y|x) = \\frac{f_{XY}(x, y)}{f_X(x)} = \\frac{f_{X|Y}(x|y)f_Y(y)}{\\lim_{-\\infty}^{\\infty}f_{X|Y}(x| y')f_Y(y')dy'}$\n",
    "\n",
    "### 3.7 Indepdence\n",
    "\n",
    "Two random variables are independent if $F_{XY}(x, y) = F_X(x)F_Y(y)$ for all values of x and y. Equivalently:\n",
    "\n",
    "* For discrete RV, $p_{XY}(x, y) = p_X(x)p_Y(y)$ for all $x\\in Val(X), y\\in Val(Y)$\n",
    "* For discrete RV, $p_{Y|X}(y|x) = p_Y(y)$ whenever $p_X(x)\\neq 0$ for all $y\\in \\mathbb{R}$\n",
    "* For continuous RV, $f_{XY}(x, y) = f_X(x)f_Y(y)$ for all $x, y \\in \\mathbb{R}$\n",
    "* For continuous RV, $f_{Y|X}(y|x) = f_Y(y)$ whenver $f_X(x) \\neq 0$ for all $y\\in\\mathbb{R}$\n",
    "\n",
    "Informally, two variables are independent if knowning the value of one of them will not change the conditional probability distribution of the other variable.\n",
    "\n",
    "#### Lemma 3.1:\n",
    "\n",
    "If $X$ and $Y$ are independent then for any subsets $A, B \\subset \\mathbb{R}$, we have:\n",
    "\n",
    "$P(X\\in A, Y\\in B) = P(X\\in A)P(Y\\in B)$\n",
    "\n",
    "This lemma can be used to prove if $X$ is independent from $Y$. If so, then any function of $X$ is independent of any function of $Y$.\n",
    "\n",
    "### 3.8 Expectation and Co-variance\n",
    "\n",
    "Suppose two random variables $X$, $Y$ and $g:\\mathbb{R}^2\\to\\mathbb{R}$ is a function of these two random variables. Then the expected value of $g$ is defined as:\n",
    "\n",
    "$\\mathbb{E}[g(X, Y)] = \\sum_{x \\in Val(X)}\\sum_{y\\in Val(Y)} g(x, y)p_{XY}(x, y)$\n",
    "\n",
    "For continous random variables $X, Y$, the analogous expression is:\n",
    "\n",
    "$\\mathbb{E}[g(X, Y)] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}g(x, y)f_{XY}(x, y)dxdy$.\n",
    "\n",
    "Can use the concept of expectation to study th relationship of the two random variables with each other. In particular, the co-variance of the two random variables can be defined as:\n",
    "\n",
    "$Cov[X,Y] = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]$\n",
    "\n",
    "Using a similar argument as before, we can rewrite as:\n",
    "\n",
    "$Cov[X,Y] = \\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$.\n",
    "\n",
    "Here, the key step in showing the equality of the two forms of covariance is in the third equality, where we use the fact that $\\mathbb{E}[X]$ and $\\mathbb{E}[Y]$ are actually constants which can be pulled out of the expectation. When $Cov[X,Y] = 0$, we say that $X$ and $Y$ are uncorrelated.\n",
    "\n",
    "#### _Properities:_\n",
    "\n",
    "* (Linearity of expectation) $\\mathbb{E}[f(X,Y) + g(X, Y)] = \\mathbb{E}[f(X,Y)] + \\mathbb{E}[g(X, Y)]$\n",
    "* $Var[X + Y] = Var[X] + Var[Y] + 2Cov[X, Y]$\n",
    "* If $X$ and $Y$ are independent, then $Cov[X, Y] = 0$\n",
    "* If $X$ and $Y$ are independent, then $\\mathbb{E}[f(X) + f(Y)] = \\mathbb{E}[f(X)]\\mathbb{E}[g(X)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks\n",
    "\n",
    "This is used to _represent_ a probablistic model. These are done through **directed acyclic graphs (DAGs)**.\n",
    "\n",
    "## Probabilistic Modeling with Bayesian Networks\n",
    "\n",
    "Recall that by the chain rule, we can write any joint probability $p$ as:\n",
    "\n",
    "$p(x_1, x_2, ..., x_n) = p(x_1)p(x_2|x_1)...p(x_n|x_{n-1},...,x_2,x_1)$.\n",
    "\n",
    "A compact Bayesian network is a distribution in which each factor on the right hnd side depends only on a small number of _ancestor variables_ $x_{A_i}$:\n",
    "\n",
    "$p(x_i|x_{i-1}, ...,x_1) = p(x_i|x_{A_i})$.\n",
    "\n",
    "For example, a model with 5 variables, we may choose to approximate the factor $p(x_5|x_4,x_3,x_2,x_1)$ with $p(x_5|x_4,x_3)$. In this case, we write $x_{A_5} = \\{x_4, x_3\\}$.\n",
    "\n",
    "When the variables are discrete, we may think of the factors $p(x_i|x_{A_i})$ as _probability tables_, in which rows correspond to assignments to $x_{A_i}$ and columns correspond to values of $x_i$; the entries contain the actual probabilities $p(x_i|x_{A_i})$. If each variables takes $d$ values and has at most $k$ ancestors, then the entire table will contain at most $O(d^{k+1})$ entries. Since we have one table per variable, the entire probability distribution can be compactly described with only $O(nd^{k+1})$ parameters (compared to $O(d^n)$ with a naive approach).\n",
    "\n",
    "### Graphical Representation\n",
    "\n",
    "Expressed as Directed Acyclic Graphs (DAGs) in which vertices correspond to variables $x_i$ and edges indicates dependency relationships. In particular we set the parents of each node to $x_i$ amd its ancestors $x_{A_i}$.\n",
    "\n",
    "As an example, consider a model of a student's grade $g$ on an exam. This grade depends on the exam's difficulty $d$ and the student's intelligence $i$; it also affects the quality $l$ of the reference letter from the professor who taught the course. The student's intelligence $i$ affects their SAT score $s$ in addition to $g$. Each variable is binary, except for $g$, which takes 3 possible values. The joint probability distribution over the 54 variables naturally factorizes as follows:\n",
    "\n",
    "$p(l, g, i, d, s) = p(l|g)p(g|i, d)p(i)p(d)p(s|i)$.\n",
    "\n",
    "Bayesian Net Model:\n",
    "<img src=\"images/dag.png\" width=\"50%\"/>\n",
    "\n",
    "Another way to interpret directed graphs is in terms of stories for how the data was generated. In the above example, to determine the quality of the reference letter, we may first sample an intelligence level and an exam difficulty; then, a student's grade is sampled given these parameters; finally, the recommendation letter is generated based on that grade.\n",
    "\n",
    "In the case of the previous _spam classification example_, we implicitly postulated that email is generated according to a two-step process: first, we choose a spam/non-spam label $y$; then we sample independently whether each word is present, conditioned on that label.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "Formally, a Bayesian network is a directed graph $G=(V, E)$ together with:\n",
    "* A random variable $x_i$ for each node $i\\in V$\n",
    "* One conditional probability distribution (CPD) $p(x_i)|x_{A_i}$ per node, specifying the probability of $x_i$ conditioned on its parents' values.\n",
    "\n",
    "Thus, a Bayesian network defines a probability distribution $p$. Conversely, we say that a probability $p$ _factorizes_ over a DAG $G$ if it can be decompased into a product of factors, as specified by $G$.\n",
    "\n",
    "It is not hard to see that a probability represented by a Bayesian network wil be valid: clearly, it will be non-negative and one can show using an induction argument (and using the fact that the CPDs are valid probabilities) that the sum over all variable assignments will be one. Conversely, we can also show by counter-example that when $G$ contains cycles, its associated probability may not sum to 1.\n",
    "\n",
    "## The Dependencies of a Baye's Net\n",
    "\n",
    "Let us use the notation $I(p)$ to denote the set of all independencies that hold for a joint distribution $p$. For example, if $p(x,y) = p(x)p(y)$, then we say that $x\\perp y\\in I(p)$.\n",
    "\n",
    "### Independencies Described By Directed Graphs\n",
    "\n",
    "The independenciescan be recovered from the graph by looking at three types of structures.\n",
    "\n",
    "Let's start by looking at a Baye's net $G$ with three nodes: $A, B, C$. In this case, $G$ essentially has only three possible structures, each of which leads to different independence assumptions. The interested reader can easily prove these results using a bit of algebra.\n",
    "\n",
    "* _Common Parent_: If $G$ is of the form $A\\leftarrow B\\to C$, and $B$ is observed, then $A\\perp C$. However, if $B$ is unobserved, then A$\\not\\perp C$. Intuitively, this stems from the fact that $B$ contains all the information that determines the outocmes of $A$ and $C$; once it is observed, there is nothing else that affects these variables' outcomes.\n",
    "\n",
    "\n",
    "* _Cascade_: If $G$ is of the form $A\\to B\\to C$, and $B$ is again observed, then, again $A \\perp C | B$. However, if $B$ is unobserved, then $A \\not\\perp C$. Intuitively, $B$ holds all the information that determines the outcome of $C$; thus, it does not matter what value $A$ takes.\n",
    "\n",
    "\n",
    "* _V-structure (also known as explaining away)_: If $G$ is of the form $A\\to C \\leftarrow B$, then knowning $C$ couples $A$ and $B$. In other words, $A\\perp B$ if $C$ is unobserved, but $A \\not\\perp B | C$ if $C$ is observed.\n",
    "\n",
    "<img src=\"images/3node_indep.png\" width=\"50%\"/>\n",
    "\n",
    "In this photo, (a, b) is cascade, (c) is common parent, (d) is v-structured.\n",
    "\n",
    "For V-structure, suppose $C$ is a Boolean variable that indicates whether our lawn is wet one morning; $A$ and $B$ are two explanations for it being wet: either it rained ($A$) or the sprinkler turned on ($B$). If we know that the grass is wet ($C$ is true) and the sprinkler didn't go on ($B$ is false), then probability that $A$ is true must be one, because that is the only other possible explanation. Hence, $A$ and $B$ are not independent given $C$.\n",
    "\n",
    "These structures can be extended to general networks by applying them recursively over any larger graph. This leads to a notion called $d$-separation (where $d$ stands for directed).\n",
    "\n",
    "We say that $Q, W$ are $d$-separated when variables $O$ are observed if they are not connected by an _active path_. An undirected path in the Bayesian Network structure $G$ is called _active_ given observed variables $O$ if for every consecutive triple of variables $X, Y, Z$ on the path, one of the following holds:\n",
    "\n",
    "* $X\\leftarrow Y\\leftarrow Z$, and $Y$ is unobserved $Y\\not\\in O$\n",
    "\n",
    "* $X\\to Y\\to Z$, and $Y$ is unobserved $Y\\not\\in O$\n",
    "\n",
    "* $X\\leftarrow Y\\to Z$, and $Y$ is unobserved $Y\\not\\in O$\n",
    "\n",
    "* $X\\to Y\\leftarrow Z$, and $Y$ or any of its descendants are observed\n",
    "\n",
    "----\n",
    "\n",
    "#### Examples\n",
    "<img src=\"images/dsep_1.png\" width=\"50%\"/>\n",
    "$X_1$ and $X_6$ are $d$-separated given $X_2, X_3$. \n",
    "\n",
    "<img src=\"images/dsep_2.png\" width=\"50%\"/>\n",
    "However, $X_2, X_3$ are not $d$-separated given $X_1$, $X_6$, because we can find an active path ($X_2, X_6, X_5, X_3$). This active path is a V-structure created when $X_6$ is observed.\n",
    "\n",
    "----\n",
    "\n",
    "The notion of $d$-separation is useful, because it lets us describe a large fraction of the dependencies that hold in our model. Let $I(G) = \\{(X\\perp Y | Z): X, Y$ are $d$-sep given $Z\\}$ be a set of variables that are $d$-separated in $G$. Intuitively, if X, Y and Y, Z are mutually depende, so are X, Z.\n",
    "\n",
    "If $p$ factorizes over $G$, then $I(G)\\subset I(p)$. In this case, we say $G$ is an _I_-map (independence map) for $p$.\n",
    "\n",
    "In other words, all the independencies encoded in $G$ are sound: variables that are $d$-separated in $G$ are truly indepdent in $p$. However, the converse is not true: a dustribution may factorize over $G$, yet have independencies that are not captured in $G$.\n",
    "\n",
    "In a way, this is almost a trivial statement. If $p(x, y) = p(x)p(y)$, then this distribution still factorizes over the graph $y\\to x$, since we can always write it as $p(x, y) = p(x|y)p(y)$ with a CPD $p(x|y)$ in which the probability of $x$ does not actually $y$. However, we can construct a graph that matches the structure of $p$ by simply removing that unnecessary edge.\n",
    "\n",
    "### When are Two Bayesian Nets I-Equivalent?\n",
    "\n",
    "We say that two Baye's nets $G_1, G_2$ are I-equivalent if they encode that same dependencies $I(G_1) = I(G_2)$.\n",
    "\n",
    "To answer this, let's return to a simple example with three variables. We say that each of the graphs below have the same _skeleton_, meaning that if we drop the directionality of the arrows, we obtain the same undirected graph in each case.\n",
    "\n",
    "<img src=\"images/3node_indep.png\" width=\"50%\"/>\n",
    "\n",
    "The cascade-type structures (a, b) are clearly symmetric and the directionality of arrows does not matter. In fact, (a, b, c) encode exactly the same dependencies/ We can change the directions of the arrows as long as we don't turn them into a V-structure (d). When we do have a V-structure, however, we cannot change any arrows: structure (d) is the only one that describes the dependency $X\\not\\perp Y | Z$. These examples provide intuitions of the following general results on I-equivalence.\n",
    "\n",
    "**Fact:** If $G, G'$ have the same skeleton and the same V-structures, then $I(G) = I(G')$.\n",
    "\n",
    "Intuitively, two graphs are I-equivalent if the $d$-separation between variables is the same. We can flip the directionality of any edge, unless it forms a V-structure, and the $d$-connectivity of the graph will be unchanged. We refer the reader to the textbook of Koller and Friedman for a full proof.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Random Fields\n",
    "\n",
    "When we can't properly represent independence assumptions with a Bayesian network, instead of introducing false independencies among the variables of our model, we must fall back to a less compact representation (which can be viewed as a graph with additional, unnecessary edges). This leads to extra, unnecessary parameters in the model, and makes it more difficult to learn these parameters and to make predictions.\n",
    "\n",
    "We can compactly model this probability distribution with _undirected_ graphs. This class of models (known as **Markov Random Fields or MRFs**) can compactly represent independence assumptions that directed models cannot represent.\n",
    "\n",
    "## Markov Random Fields\n",
    "\n",
    "For example, suppose that we are modeling voting preferences among person $A, B, C, D$. Lets say that $(A, B), (B, C), (C, D), (D, A)$ are friends and friends tend to have similar voting prefernces. These influences can be naturally represented by an undirected graph.\n",
    "\n",
    "<img src=\"images/mrf.png\" width=\"50%\"/>\n",
    "\n",
    "One way to define a probability over the joint voting decision of $A,B,C,D$ is to assign scores to each assignment to these variables and then define a probability as a normalized score. A score can be any function, but in our case, we will define it to be of the form:\n",
    "\n",
    "$\\widetilde{p}(A,B,C,D) = \\phi(A,B)\\phi(B,C)\\phi(C,D)\\phi(D,A)$\n",
    "\n",
    "where $\\phi(X,Y)$ is a factor that assigns more weight to consistent votes among friends $X,Y$, eg:\n",
    "\n",
    "$\\phi(X,Y) = \\begin{cases} \n",
    "            10, & if X = Y = 1 \\\\\n",
    "            5, & if X = Y = 0 \\\\\n",
    "            1, & otherwise\n",
    "       \\end{cases}$\n",
    "       \n",
    "The factors in the unnormalized distribution are often referred to as _factors_. The final probability is then defined as:\n",
    "\n",
    "$p(A,B,C,D) = \\frac{1}{Z}\\widetilde{p}(A,B,C,D)$\n",
    "\n",
    "where $Z = \\sum_{A,B,C,D}\\widetilde{p}(A,B,C,D)$ is a normalizing constant that ensures that the distribution sums to one.\n",
    "\n",
    "When normalized, we can view $\\phi (A,B)$ as an interaction that pushes $B$'s vote closer to $A$, etc.\n",
    "\n",
    "Note: Unlike the directed case, we are not saying anything about how one variable is generated from another set of variables (as a conditional probability distribution would do). We simply indicate a level of coupling between dependent variables in the graph. In a sense, this requires less prior knowledge, as we no longer have to specify a full generative story of how the vote of $B$ is constructed from the vote of $A$ (which we would need to do if we had a $P(B|A)$ factor). Instead we simply identify dependent variables and define the strength of their interactions; this in turn defines an energy landscape over the space of possible assignments and we convert this energy to a probability via the normalization constant.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "A Markov Random Field (MRF) is a probability distribution $p$ over variables $x_1, ..., x_n$ defined by an _undirected_ graph $G$ in which nodes correspond to variables $x_i$. The probability $p$ has the form:\n",
    "\n",
    "$p(x_1,...,x_n)=\\frac{1}{Z}\\prod_{c\\in C}\\phi_c(x_c)$\n",
    "\n",
    "where $C$ denotes the set of _cliques_ (ie. fully connected subgraphs) of $G$. The value:\n",
    "\n",
    "$Z = \\sum_{x_1,...,x_n}\\prod_{c\\in C}\\phi_c(x_c)$\n",
    "\n",
    "is a normalizing constant that ensures that the distribution sums to one.\n",
    "\n",
    "Thus, given a graph $G$, our probability distribution may contain factors whose scope is any clique in $G$, which can be a single node, an edge, a triangle, etc. Note that we do not need to specify a factor over each edge (which is a clique of two nodes). However, we chose not to specify any unary factors, ie. cliques over single nodes.\n",
    "\n",
    "### Comparison to Bayesian Networks\n",
    "\n",
    "In our earlier voting example, we had a distribution over $A,B,C,D$ that satisfied $A\\perp C | \\{B,D\\}$ and $B\\perp D | \\{A,C\\}$ (because only friends directly influence a person's vote). We can easily check by counter-example that these independencies cannot be perfectly represented by a Bayesian network. However, the MRF turns out to be a perfect map for this distribution.\n",
    "\n",
    "<img src=\"images/mrf2.png\" width=\"50%\"/>\n",
    "\n",
    "More generally, MRFs have several advantages over directed models:\n",
    "* They can be applied to a wider range of problems in which these is no natural directionality associated with variabel dependencies\n",
    "\n",
    "\n",
    "* Undirected graphs can succinctly express certain dependencies that Bayesian nets cannot easily describe (althrough the converse is also true)\n",
    "\n",
    "They also possess several important drawbacks:\n",
    "\n",
    "* Computing the normalization constant $Z$ requires summing over a potentially exponential number of assignments. We will see that in the general case, this will be NP-hard; thus many undirected models will be intractable and will require approximation techniques.\n",
    "\n",
    "\n",
    "* Undirected models maybe difficult to interpret\n",
    "\n",
    "\n",
    "* It is much easier to generate data from a Bayesian network, which is important in some applications\n",
    "\n",
    "It is not hard to see that Bayesian networks are a special case of MRFs with a very specific type of clique factor (one that corresponds to a conditional probability distribution and implies a directed acyclic structure in the graph), and a normalizing constant of one. In particular, if we take a directed graph $G$ and add side edges to all parents of a given node (and removing their directionality), then the CPDs (seen as factors over a variable and its ancestors) factorize over the rsulting undirected graph. The resulting process is called _moralization_.\n",
    "\n",
    "<img src=\"images/moralization.png\" width=\"50%\"/>\n",
    "\n",
    "Thus, MRFs have more power than Bayesian networks, but are more difficult to deal with computationally. A general rule of thumb is to use Bayesian netowrks whenever possible and only switch to MRFs if there is no natural way to model the problem with a directed graph (like in our voting example).\n",
    "\n",
    "### Independencies in Markov Random Fields\n",
    "\n",
    "Recall that in the case of Bayesian networks, we defined a set of independencies $I(G)$ that were described by a directed graph $G$, and showed how these describe true independencies that must hold in a distribution $p$ that factorizes over the directed graph ie. $I(G)\\subset I(p)$\n",
    "\n",
    "In the case of undirected MRFs, variables $x, y$ are dependent if they are connecte3d by a path of unobserved variables. However, if $x$'s neighbors are all observed, then $x$ is independent of all the other variables, since they influence $x$ only via its neighbors.\n",
    "\n",
    "In particular, if a set of observed variables forms a cut-set between two halves of the graph, then variables in one half are independent from the ones in the other.\n",
    "\n",
    "<img src=\"images/cutset.png\" width=\"50%\">\n",
    "\n",
    "In the image below, the node $X$ is independent from the rest of the graph given its neighbors (which are referred to as the **Markov Blanket** of $X$).\n",
    "\n",
    "<img src=\"images/markovblanket.png\" width=\"50%\">\n",
    "\n",
    "Formally speaking, we define the _Markov blanket $U$_ of a variable $X$ as the minimal set of nodes such that $X$ is independent from the rest of the graph if $U$ is observed. ie.e $X\\perp (X-\\{X\\}-U)|U$. This notion holds for both directed and undirected models, but in the undirected case, the Markov blanket turns out to simply equal a node's neighborhood.\n",
    "\n",
    "In the directed case, we found that $I(G)\\subset I(p)$ but there were distributions $p$ whose independencies could not be described by $G$. In the undirected case, the same holds. For example, consider a probability described by a directed v-structure (ie. the explaining away phenomenon). The undirected model cannot describe the independence assumption $X\\perp Y$.\n",
    "\n",
    "<img src=\"images/mrf-bn-comparison.png\" width=\"50%\">\n",
    "\n",
    "## Factor Graphs\n",
    "\n",
    "It is often useful to view MRFs in a way where factors and variables are explicit and separate in the representation. A factor graph is one such way to do this. A factor graph is a bipartite graph (a graph whose verticies can be divided into two disjoint and independent sets) where one group is the variables in the distribution being modeled, and the other group is the factors defined on these variables. Edges go between factors and variables that those factors depend on.\n",
    "\n",
    "<img src=\"images/factor-graph.png\" width=\"50%\">\n",
    "\n",
    "The image above is an example of a factor graph where $f$'s are factors and $X$'s are variables.\n",
    "\n",
    "This view allows us to more readily see the factor dependencies between variables, and later we'll see it allows us to compute some probability distributions more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Elimination\n",
    "\n",
    "Given a probabilistic model (such as Baye's Net or MRF), we are interested in using it to answer useful questions. For example, determining if an email is spam or not. Will be focusing on two types of questions:\n",
    "\n",
    "* _Marginal Inference:_ what is the probability of a given variable in our model after we sum everything else out (eg. probability of spam vs. non-spam)\n",
    "\n",
    "$p(y=1)=\\sum_{x_1}\\sum_{x_2}...\\sum_{x_n}p(y=1,x_1, x_2, ..., x_n)$ \n",
    "\n",
    "\n",
    "* _Maximum a posteriori (MAP) Inference:_ what is the most likely assignment to the variables in the model (possibly conditioned on evidence)\n",
    "\n",
    "$max_{x_1,...,x_n}p(y=1, x_1, ..., x_n)$\n",
    "\n",
    "Inference is a challenging task. For many probabilities of interest, it is NP-hard to answer any of these questions. Crucially, whether inference is tractable depends on the structure of the graph that describes that probability. If a problem is intractable, we are still able to obtain useful answers via approximate inference methods.\n",
    "\n",
    "We will assume for the rest of the chapter that $x_i$ are discrete variabels taking $k$ possible values each.\n",
    "\n",
    "## An Illustrative Example\n",
    "\n",
    "Consider the problem of marginal inference. Suppose for simplicity that we are given a chain Bayesian network of the form:\n",
    "\n",
    "$p(x_1, ..., x_n)=p(x_1)\\prod_{i=2}^n p(x_i|x_{i-1})$\n",
    "\n",
    "We are interested in computing the marginal probability $p(x_n)$. The naive way of calcuating this is to sum the probability over all $k^{n-1}$ assignments to $x_1,...,x_{n-1}$:\n",
    "\n",
    "$p(x_n)=\\sum_{x_1}\\sum_{x_2}...\\sum_{x_n}p(x_1, x_2, ..., x_n)$ \n",
    "\n",
    "However, we can do much better by levaging the factorization of our probability distribution. We may rewrite the sum in a way that \"pushes in\" certain variables deeper into the product.\n",
    "\n",
    "$p(x_1, ..., x_n)=p(x_1)\\prod_{i=2}^n p(x_i|x_{i-1})$\n",
    "$= \\sum_{x_{n-1}}p(x_n|x_{n-1})\\sum_{x_{n-2}}p(x_{n-1}|x_{n-2})...\\sum_{x_1}p(x_2|x_1)$\n",
    "\n",
    "We sum the inner terms first, starting from $x_1$ and ending with $x_{n-1}$. Concretely, we start by computing an intermediary _factor_ $\\tau (x_2)=\\sum_{x_1}p(x_2|x_1)p(x_1)$ by summing out $x_1$. This takes $O(k^2)$ time because we must sum over $x_1$ for each assignment to $x_2$. THe resulting factor $\\tau(x_2)$ can be thought of as a table of $k$ values (though not necessarily probabilities), with one entry for each assignment to $x_2$ (just as factor $p(x_1)$ can be represented as a table). We may then rewrite the marginal probability using $\\tau$ as:\n",
    "\n",
    "$p(x_n) = \\sum_{x_{n-1}}p(x_n|x_{n-1})\\sum_{x_{n-2}}p(x_{n-1}|x_{n-2})...\\sum_{x_2}p(x_3|x_2)\\tau(x_2)$\n",
    "\n",
    "Note that this has the same form as the initial expression, except that we are summing over one fewer variable (this technique is a special case of DP). We may therefore compute another factor $\\tau (x_3)=\\sum_{x_2}p(x_3|x_2)p(x_2)$, and repeat the process until we are only left with $x_n$. Since each step takes $O(k^2)$ time and we are performing $O(n)$ steps, inference now takes $O(nk^2)$ time, which is much better than our naive $O(k^n)$.\n",
    "\n",
    "Also, at each time, we are _eliminating_ a variable, which gives the algorithm its name.\n",
    "\n",
    "## Elimiating Variables\n",
    "\n",
    "The general form of variable elimination algorithm.\n",
    "\n",
    "### Factors\n",
    "\n",
    "We will assume that we are given a graphical model as a product of factors:\n",
    "\n",
    "$p(x_1, ..., x_n) = \\prod_{c\\in C}\\phi_c (x_c)$\n",
    "\n",
    "Recall that we can view a factor as a multi-dimensional table assigning a value to each assignment of a set of variables $x_c$. In the context of a Bayesian network, the factors correspond to conditional probability distributions; however, this definition also makes our algorithm equally applicable to Markov Random Fields. In this latter case, the factors encode an unnormalized distribution; to compute marginals, we first calculate the partition function (also using variable elimination), then we compute marginals using the unnormalized distributions, and finally we divide the result by the partition constant to construct a valid marginal probability.\n",
    "\n",
    "### Factor Operations\n",
    "\n",
    "The variable elimination algorithm will repeatedly perform two factor operations: product and marginalization. We have been implicitly performing these operations in our chain example.\n",
    "\n",
    "The factor product operation simply defines the product $\\phi_3 := \\phi_1 \\times \\phi_2$ of two factors $\\phi_1, \\phi_2$ as:\n",
    "\n",
    "$\\phi_3(x_c) = \\phi_1(x_c^{(1)}) \\times \\phi_2(x_c^{(2)})$\n",
    "\n",
    "The scope of $\\phi_3$ is defined as the union of the variables in the scopes of $\\phi_1, \\phi_2$; also $x_c^{(i)}$ denotes an assignment to the variables in the scope of $\\phi_i$ defined by the restriction of $x_c$ to that scope. For example, we define:\n",
    "\n",
    "$\\phi_3(a,b,c):=\\phi_1(a,b)\\times\\phi_2(b,c)$\n",
    "\n",
    "Next, the marginalization operation \"locally\" eliminates a set of variables from a factor. If we have a factor $\\phi(X,Y)$ over two sets of variables $X,Y$ marginalizing $Y$ produces a new factor:\n",
    "\n",
    "$\\tau(x) = \\sum_y \\phi(x,y)$\n",
    "\n",
    "where the sum is over all joint assignments to the set of variables $Y$.\n",
    "\n",
    "<img src=\"images/marginalization.png\" width=\"50%\">\n",
    "\n",
    "We use $\\tau$ to refer to the marginalized factor. It is important to understand that this factor does not necessarily correspond to a probability distribution, even if $\\phi$ was a CPD.\n",
    "\n",
    "### Orderings\n",
    "\n",
    "Finally, the variable elimination algorithm requires an ordering over the variables according to which variables will be \"eliminated\". In our chain example, we took the ordering implied by the DAG. It is important to note that:\n",
    "\n",
    "* Different orderings may dramatically alter the running time of the variable elimination algorithm.\n",
    "\n",
    "\n",
    "* It is NP-hard to find the best ordering.\n",
    "\n",
    "We will come back to these complications later, but for now, let the ordering be fixed.\n",
    "\n",
    "### The Variable Elimination (VE) Algorithm\n",
    "\n",
    "Essentially, we loop over the variables as ordered by $O$ and eliminatte them in that ordering. Intuitively, this corresponds to choosing a sum and \"pushing it in\" as far as possible inside the product of the factors, as we did in the chain example.\n",
    "\n",
    "More formally, for each variable $X_i$ (ordered according to $O$),\n",
    "\n",
    "1. Mulitply all factors $\\Phi_i$ containing $X_i$\n",
    "\n",
    "2. Marginalize out $X_i$ to obtain a new factor $\\tau$\n",
    "\n",
    "3. Replace the factors $\\Phi_i$ with $\\tau$\n",
    "\n",
    "### Examples\n",
    "\n",
    "Let's try to understand what these steps correspond to in our chain example. IN that case, the chosen ordering was $x_1, x_2,...,x_{n-1}$. Starting with $x_1$, we collected all the factors involving $x_1$, which were $p(x_1)$ and $p(x_2|x_1)$. We then used them to construct a new factor $\\tau(x_2)=\\sum_{x_1}p(x_2|x_1)p(x_1)$; then we eliminate $x_1$ from that factor to produce $\\tau$. Then, we repeat the same procedure for $x_2$, except that the factors are now $p(x_3|x_2), \\tau(x_2)$.\n",
    "\n",
    "For a slightly more complex example, recall the graphical model of a student's grade that we introduced earlier. \n",
    "\n",
    "<img src=\"images/dag.png\" width=\"50%\"/>\n",
    "\n",
    "The probaiblity specified by the model is of the form:\n",
    "\n",
    "$p(l,g,i,d,s)=p(l|g)p(s|i)p(i)p(g|i,d)p(d)$.\n",
    "\n",
    "Lets suppose that we are computing $p(l)$ and are eliminating variables in their topological ordering in the graph. First we eliminate $d$, which corresponds to creating a new factor $\\tau_1(g, i)=\\sum_dp(g|i,d)p(d)$. Next, we eliminate $i$ to produce a factor $\\tau_2(g, s) = \\sum_i\\tau_1(g, i)p(i)p(s|i)$; then we eliminate $s$ yielding $\\tau_3(g)=\\sum_s\\tau_2(g,s)$, and so on. Note that these operations are equivalent to summing out the factored probability distributions as follows:\n",
    "\n",
    "$p(l)=\\sum_gp(l|g)\\sum_s\\sum_ip(s|i)p(i)\\sum_dp(g|i,d)p(d)$\n",
    "\n",
    "Note that this example requires computing at most $k^3$ operations per step, since each factor is at most over 2 variables, and one variable is summed out at each step (the dimensionality $k$ in this example is either 2 or 3).\n",
    "\n",
    "## Introducing Evidence\n",
    "\n",
    "A closely related and equally important problem is computing conditional probabilities of the form:\n",
    "\n",
    "$P(Y|E=e) = \\frac{P(Y,E=e)}{P(E=e)}$\n",
    "\n",
    "where P(X,Y,E) is a probability distribution, over sets of query variabels $Y$, observed evidence variables $E$, and unobserved variables $X$.\n",
    "\n",
    "We can comnpute this probability by performing variable elimination once on $P(Y, E=e)$ and then once more on $P(E=e)$.\n",
    "\n",
    "To compute $P(Y, E=e)$, we simply take every factor $\\phi(X', Y', E')$ which has scope over variables $E'\\subset E$ that are also found in $E$, and we set their values as specified by $e$. Then we perform standard variable elimination over $X$ to obtain a factor over only $Y$.\n",
    "\n",
    "## Running Time of Variable Elimination\n",
    "\n",
    "The runtime depends heavily on the structure of the graph.\n",
    "\n",
    "In the previous example, suppose we eliminated $g$ first. Then we would have had to transform the factors $p(g |i, d), \\phi(l|g)$ into a big factor $\\tau(d, i, l)$ over 3 variables, which would require $O(k^4)$ time to compute: $k$ times for each of three conditional variables, and $k$ times for each value of $g$. If we had a factor $S \\implies G$, then we would have had to eliminate $p(g|s)$ as well, producing a single giant factor $\\tau(d, i, l, s)$ in $O(k^5)$ time. Then, eliminating any variable from this factor would require almost as much work as if we had started with the original distribution, since all the vairables have become coupled.\n",
    "\n",
    "Clearly, some orderings are more efficient than others. In fact, the running time of Variable Elimination is $O(nk^{M+1})$, where $M$ is the maximum size of any factor $\\tau$ formed during the elimination process and $n$ is the number of variables.\n",
    "\n",
    "### Choosing Variable Elimination Orderings\n",
    "\n",
    "Unfortunately, chhosing the optimal VE ordering is an NP-hard problem. However, in practice, we may resort to the following heuristics:\n",
    "\n",
    "* _Min-neighbors:_ Choose a variable with the fewest dependent variables\n",
    "\n",
    "\n",
    "* _Min-weight:_ Choose variables to minimize the product of the cardinalities of its dependent variables.\n",
    "\n",
    "\n",
    "* _Min-fill:_ Choose verticies to minimize the size of the factor that will be added to the graph.\n",
    "\n",
    "These methods often result in reasonality good performance in many interesting settings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP Inference\n",
    "\n",
    "MAP inference in a graphical model $p$ corresponds to the following optimization problem:\n",
    "\n",
    "$max_{x_1,...,x_n}p(x_1,...,x_n)$\n",
    "\n",
    "The framework we have introduced for marginal inference now lets us easily perform MAP inference as well. The key observation is that the sum and max operators both distribute over products. Thus, replacing sums in marginal inference with maxes, we are able to solve the MAP inference problem.\n",
    "\n",
    "For example, we may compute the partition function of a chain MRF as follows:\n",
    "\n",
    "$Z = \\sum_{x_1}...\\sum{x_n}\\phi(x_1)\\prod_{i=2}^n\\phi(x_i, x_{i-1})$\n",
    "$=\\sum_{x_n}\\sum_{x_{n-1}}\\phi(x_n, x_{n-1})\\sum_{x_{n-2}}\\phi(x_{n-1},x_{n-2})...\\sum_{x_1}\\phi(x_2,x_1)\\phi(x_1)$\n",
    "\n",
    "To compute the maximum $\\bar{p}^*$ of $\\bar{p}(x_1, ..., x_n)$, we simply replace sums with maxes, ie.\n",
    "\n",
    "$\\tilde{p}^* = max_{x_1}...max_{x_n}\\phi(x_1)\\prod_{i=2}^n\\phi(x_i, x_{i-1})$\n",
    "$= max_{x_n}max_{x_{n-1}}\\phi(x_n, x_{n-1})max_{x_{n-2}}\\phi(x_{n-1}, x_{n-2})...max_{x_1}\\phi(x_2, x_1)\\phi(x_1)$\n",
    "\n",
    "Since both problems decompose in the same way, we may reuse all of the machinery developed for marginal inference and apply it directed to MAP inference. This also applies to factor trees.\n",
    "\n",
    "There is a small caveat in that we often want to not just maximize value of a distribution, ie. $max_xp(x)$, but also its most probable assignment, ie. $arg max_xp(x)$. This problem can be easily solved by keeping _back-pointers_ during the optimization procedure. For instance, in the above example, we would keep a backpointer to the best assignemnt to $x_1$ given to each assignment to $x_2$, a pointer to the best assignment to $x_2$ given each assignment to x_3, and so on.\n",
    "\n",
    "## Challenges of MAP Inference\n",
    "\n",
    "Since we are maximizing the variable of $p(x)$ youi will usually see the log form used, since maximizing the logarithm of a function will yield the same value as maximizing the function itself.\n",
    "\n",
    "$max_xlog p(x) = max_x\\sum_c\\theta_c(x_c)-log Z$\n",
    "\n",
    "where $\\theta_c(x_c) = log\\phi_c(x_c)$. Since $Z$ is a constant this lets us easily ignore that term.\n",
    "\n",
    "In a way, MAP inference is easier than margnial inference. One reason for this is that the intractable partition constant $log Z$ does not depend on $x$ and can be ignored:\n",
    "\n",
    "$arg max_x \\sum_c\\theta_c(x_c)$\n",
    "\n",
    "Marginal inference can also be seen as computing and summing all assignments to the model, one of which is the MAP assignment. If we replace summation with maximization, we can also find the assignment with the highest probaiblity; however, there exist more efficient methods than this sort of enumeration-based approach. The notes on **Belief Propagation** show briefly how to solve this problem exactly within the same message passing framework as marginal inference. Here we look at more efficient specialized methods.\n",
    "\n",
    "Nonetheless, the MAP problem is easier than general inference, in the sense that there are some models in which MAP inference can be solved in polynomial time, while general inference is NP-hard.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Many interesting examples of MAP inference are instances of _structured prediction_, which involves doing inference in a conditional random field (CRF) model $p(y|x)$:\n",
    "\n",
    "$arg max_y log p(y|x) = arg max_y \\sum_c \\theta_c(y_c, x_c)$\n",
    "\n",
    "One example is handwriting recognizion, in which we are given images $x_i\\in [0, 1]^{d\\times d}$ of characters in the form of pixel matrices; MAP inference in this setting amounts to jointly recognizing the most likely word $(y_i)_{i=1}^n$ encoded by the images.\n",
    "\n",
    "Another example of MAP inference is image segmentation; here, we are interested in locating an entity in an image and label all its pixels. Our input $x\\in [0, 1]^{d\\times d}$ is a matrix of image pixels, and our task is to predict the label $y\\in{0, 1}^{d\\times d}$, indicating whether each pixel encodes the object we want to recover. Intuitively, neighboring pixels should have similar values in $y$, ie. pixels associated with the horse should form one continuous blob, rather than white noise.\n",
    "\n",
    "<img src=\"images/imagesegmentation.png\" width=\"50%\">\n",
    "\n",
    "This prior knowledge can be naturally modeled in the language of graphical models via a **Potts model**. As in our first example, we can introduce potential $\\phi(y_i, x)$ that encode the likelihood that any given pixel is from our subject. We then augment them with pairwise potentials $\\phi(y_i, y_j)$ for neighboring $y_i, y_j$, which will encourage adjacent $y$'s to have the same value with higher probability.\n",
    "\n",
    "## Solution Methods\n",
    "\n",
    "Formulating the problem doesn't provide an efficient solution directly.\n",
    "\n",
    "### Graph Cuts\n",
    "\n",
    "A _graph cut_ of an undirected graph $G = (V, \\mathcal{E})$ is a partition of $V$ into 2 disjoint sets $V_s, V_t$. When each edge $(v_1, v_2) \\in \\mathcal{E}$ is associated with a non-negative cost $cost(v_1, v_2)$, the cost of a graph cut is the sum of the costs of the edges that cross between the two partitions:\n",
    "\n",
    "$cost(V_s, V_t) = \\sum_{v_1\\in V_s, v_2\\in V_t} cost(v_1, v_2)$\n",
    "\n",
    "The _min-cut_ problem is to find the partition $V_s, V_t$ that minimizes the cost of the graph cut. The fastest algorithms for computing min-cuts in a graph take $O(|\\mathcal{E}||V|log|V|)$ or $O(|V^3|)$ time, and we refer readers to algorithms textbooks for details on their implementations.\n",
    "\n",
    "MAP inference can be reduced to the min-cut problem in certain restricted cases of MRFs with binary variables.\n",
    "\n",
    "### Linear Programming-based Approaches\n",
    "\n",
    "An approximate approach to computing the MAP values is to use Integer Linear Programming or Lagrangian optimization by introducting:\n",
    "\n",
    "* indicator variables for each variable in the PGM\n",
    "\n",
    "\n",
    "* indicator variables for each edge (or clique) in the PGM\n",
    "\n",
    "\n",
    "* constraints on consistent values in cliques, probabilities summing to 1, etc. Such a system is then solved using one of many linear optimization methods. Expensive for large systems but solution methods becoming more powerful all the time.\n",
    "\n",
    "### Local Search\n",
    "\n",
    "A more heuristic-type solution consists in starting with an arbitrary assignment and perform \"moves\" on the joint assignment that locally increase the probability. This technique has no guarentees; however, we can often use prior knowledge to come up with highly effective moves. Therefore, in practice, lcoal search may perform extremely well. A popular example is BFGS algorithm which is a quasi-Newton method for performing local search. This method approximates the second derivative of the underlying system and searches for a point where it is zero (ie. the minimum) to stop searching. This is the default for the _find_MAP_ function in _PyMC_ we will use later.\n",
    "\n",
    "### Branch and Bound\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
